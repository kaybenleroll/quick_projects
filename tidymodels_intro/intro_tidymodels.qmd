---
title: "Introduction to tidymodels"
author: "Mick Cooney <mickcooney@gmail.com>"
format:
  html:
    light: cosmo
    dark: darkly
    embed-resources: true
    number-sections: true
    toc: true
    toc-depth: 2
    toc-location: left
editor: source
---



```{r knit_opts}
#| include: false


library(conflicted)
library(tidyverse)
library(magrittr)
library(rlang)
library(scales)
library(cowplot)
library(tidymodels)
library(broom.mixed)     # for converting bayesian models to tidy tibbles
library(dotwhisker)      # for visualizing regression results
library(nycflights13)    # for flight data
library(skimr)           # for variable summaries
library(modeldata)       # for the cells data
library(rpart.plot)      # for visualizing a decision tree
library(vip)             # for variable importance plots


source("lib_utils.R")

conflict_lst <- resolve_conflicts(
  c("xml2", "magrittr", "rlang", "dplyr", "readr", "purrr", "ggplot2")
  )

options(
  width = 80L,
  warn  = 1,
  mc.cores = parallel::detectCores()
  )

theme_set(theme_cowplot())

set.seed(42)
```


# Workshop 1 - Build a Model

## Retrieve and Load the Data

```{r setting_up_urchins_data}
#| echo: true


# Data were assembled for a tutorial 
# at https://www.flutterbys.com.au/stats/tut/tut7.5a.html
urchins_tbl <- read_csv(
    "https://tidymodels.org/start/models/urchins.csv"
    ) %>% 
  # Change the names to be a little more verbose
  setNames(
    c("food_regime", "initial_volume", "width")
    ) %>% 
  # Factors are very helpful for modeling, so we convert one column
  mutate(
    food_regime = factor(food_regime, levels = c("Initial", "Low", "High"))
    )
```


## Plot the Data


```{r plot_urchin_data}
#| echo: true


ggplot(urchins_tbl,
    aes(x = initial_volume, y = width, group = food_regime, col = food_regime)
    ) + 
  geom_point() + 
  geom_smooth(method = lm, se = FALSE) +
  scale_color_viridis_d(option = "plasma", end = .7)
#> `geom_smooth()` using formula 'y ~ x'

```



## Fit First Model

```{r fit_first_model}
#| echo: true


lm_fit <- linear_reg() %>% 
  fit(
    width ~ initial_volume * food_regime,
    data = urchins_tbl
    )

lm_fit %>% print()

lm_fit %>% tidy()

lm_fit %>%
  tidy() %>%
  dwplot(
    dot_args = list(size = 2, color = "black"),
    whisker_args = list(color = "black"),
    vline = geom_vline(xintercept = 0, colour = "grey50", linetype = 2)
    )
```

## Predit From First Model

```{r predict_from_first_model}
#| echo: true


new_points_tbl <- expand.grid(
    initial_volume = 20, 
    food_regime = c("Initial", "Low", "High")
    )

new_points_tbl


mean_pred_tbl <- predict(
  lm_fit,
  new_data = new_points_tbl
  )

mean_pred_tbl %>% print()


conf_int_pred_tbl <- predict(
  lm_fit, 
  new_data = new_points_tbl,
  type = "conf_int"
  )

conf_int_pred_tbl %>% print()


plot_data_tbl <- new_points_tbl %>% 
  bind_cols(mean_pred_tbl) %>% 
  bind_cols(conf_int_pred_tbl)

# and plot:
ggplot(plot_data_tbl, aes(x = food_regime)) + 
  geom_point(aes(y = .pred)) + 
  geom_errorbar(
    aes(ymin = .pred_lower, ymax = .pred_upper),
    width = .2) + 
  labs(y = "urchin size")
```


## Fit with Stan Engine

```{r fit_lm_stan_engine}
#| echo: true


# set the prior distribution
prior_dist <- rstanarm::student_t(df = 1)

set.seed(123)

# make the parsnip model
bayes_mod <- linear_reg() %>% 
  set_engine(
    "stan", 
    prior_intercept = prior_dist, 
    prior = prior_dist
    ) 

# train the model
bayes_fit <- bayes_mod %>% 
  fit(
    width ~ initial_volume * food_regime,
    data = urchins_tbl
    )

print(bayes_fit, digits = 5)

tidy(bayes_fit, conf.int = TRUE)
```



```{r predict_bayes_reg}
#| echo: true

bayes_plot_data <- new_points_tbl %>% 
  bind_cols(predict(bayes_fit, new_data = new_points_tbl)) %>% 
  bind_cols(predict(bayes_fit, new_data = new_points_tbl, type = "conf_int"))

ggplot(bayes_plot_data, aes(x = food_regime)) + 
  geom_point(aes(y = .pred)) + 
  geom_errorbar(aes(ymin = .pred_lower, ymax = .pred_upper), width = .2) + 
  labs(y = "urchin size") + 
  ggtitle("Bayesian model with t(1) prior distribution")

```


## Extra Code

```{r}
urchins_tbl %>% 
  group_by(food_regime) %>% 
  summarize(
    med_vol = median(initial_volume)
    )

bayes_mod %>% 
  fit(
    width ~ initial_volume * food_regime,
    data = urchins_tbl
    )

ggplot(urchins_tbl,
    aes(initial_volume, width)) +      # returns a ggplot object 
  geom_jitter() +                         # same
  geom_smooth(method = lm, se = FALSE) +  # same                    
  labs(x = "Volume", y = "Width")         # etc
```



# Workshop 2 - Preprocess Data



```{r load_flight_data}
#| echo: true


set.seed(123)

flight_data_tbl <- flights %>% 
  mutate(
    # Convert the arrival delay to a factor
    arr_delay = ifelse(arr_delay >= 30, "late", "on_time"),
    arr_delay = factor(arr_delay),
    
    # We will use the date (not date-time) in the recipe below
    date = lubridate::as_date(time_hour)
    ) %>% 

  # Include the weather data
  inner_join(weather, by = c("origin", "time_hour")) %>% 
  
  # Only retain the specific columns we will use
  select(
    dep_time, flight, origin, dest, air_time, distance, carrier, date,
    arr_delay, time_hour
    ) %>% 
  
  # Exclude missing data
  na.omit() %>%
  
  # For creating models, it is better to have qualitative columns
  # encoded as factors (instead of character strings)
  mutate_if(is.character, as.factor)

flight_data_tbl %>% glimpse()
```


```{r show_flight_data_skim}
#| echo: true


flight_data_tbl %>% 
  count(arr_delay) %>% 
  mutate(
    prop = n / sum(n)
    )

flight_data_tbl %>% 
  skimr::skim(dest, carrier)
```


## Create Test / Train Split

```{r create_data_splits}
#| echo: true


# Fix the random numbers by setting the seed 
# This enables the analysis to be reproducible when random numbers are used 
set.seed(222)

# Put 3/4 of the data into the training set 
data_split <- initial_split(flight_data_tbl, prop = 0.75)

# Create data frames for the two sets:
train_data_tbl <- training(data_split)
test_data_tbl  <- testing(data_split)
```


## Create Recipes and Roles

```{r create_recipes_roles}
#| echo: true


flights_recipe <- recipe(
    arr_delay ~ .,
    data = train_data_tbl
    ) %>%
  update_role(flight, time_hour, new_role = "ID")


summary(flights_recipe)

```

We now want to add some additional fields

```{r create_recipe_additional_variables}
#| echo: true


flights_recipe <- recipe(
    arr_delay ~ .,
    data = train_data_tbl
    ) %>%
  update_role(flight, time_hour, new_role = "ID") %>%
  step_date(date, features = c("dow", "month")) %>%               
  step_holiday(
    date,
    holidays = timeDate::listHolidays("US"),
    keep_original_cols = FALSE
    ) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_zv(all_predictors())
```


## Fit Model with Recipe


```{r create_workflow_fit_model_recipe}
#| echo: true


lr_mod <- logistic_reg() %>% 
  set_engine("glm")

flights_wflow <- workflow() %>% 
  add_model(lr_mod) %>% 
  add_recipe(flights_recipe)

flights_wflow %>% print()


flights_fit <- flights_wflow %>% 
  fit(data = train_data_tbl)

flights_fit %>% print()
```


```{r extract_fit_parameters}
#| echo: true


flights_fit %>% 
  extract_fit_parsnip() %>% 
  tidy()
```


## Predict with Workflow


```{r create_test_data_predictions}
#| echo: true


flights_fit %>% predict(test_data_tbl)

flights_aug_tbl <- augment(flights_fit, test_data_tbl)

# The data look like: 
flights_aug_tbl %>%
  select(arr_delay, time_hour, flight, .pred_class, .pred_on_time)
```


```{r construct_prediction_roc}
#| echo: true


flights_aug_tbl %>% 
  roc_curve(truth = arr_delay, .pred_late) %>% 
  autoplot()


flights_aug_tbl %>% 
  roc_auc(truth = arr_delay, .pred_late)
```


Check how useful the recipe has been.

```{r compare_with_basic_recipe}
#| echo: true


workflow() %>% 
  add_model(lr_mod) %>% 
  add_formula(arr_delay ~ .) %>%
  fit(data = train_data_tbl %>% select(-flight, -time_hour)) %>%
  augment(test_data_tbl) %>%
  roc_auc(truth = arr_delay, .pred_late)
```



# Workshop 3 - Evaluate Models with Resampling


## Load Data and Create Stratified Split

```{r load_cells_data}
#| echo: true


data(cells, package = "modeldata")

cells_tbl <- cells %>% as_tibble()

cells_tbl %>% glimpse()
```


We now want to segment the data. This data was pre-segmented via the `case`
field, but we will create our own splits.


```{r create_cells_test_train_splits}
#| echo: true


set.seed(123)

cell_split <- initial_split(
  cells %>% select(-case), 
  strata = class
  )


cell_train_tbl <- training(cell_split)
cell_test_tbl  <- testing(cell_split)

# training set proportions by class
cell_train_tbl %>% 
  count(class) %>% 
  mutate(
    prop = n / sum(n)
    )


# test set proportions by class
cell_test_tbl %>% 
  count(class) %>% 
  mutate(
    prop = n / sum(n)
    )
```


## Construct Random Forest Model

We initially want to fit the Random Forest model for this classification

```{r construct_random_forest}
#| echo: true


rf_mod <- rand_forest(trees = 1000) %>% 
  set_engine("ranger") %>% 
  set_mode("classification")


set.seed(234)

rf_fit <- rf_mod %>% 
  fit(
    class ~ .,
    data = cell_train_tbl
    )

rf_fit %>% print()
```



```{r create_rf_training_predictions}
#| echo: true


rf_training_pred_tbl <- predict(rf_fit, cell_train_tbl) %>% 
  bind_cols(predict(rf_fit, cell_train_tbl, type = "prob")) %>% 
  # Add the true outcome data back in
  bind_cols(cell_train_tbl %>% select(class))

rf_training_pred_tbl %>% glimpse()



rf_training_pred_tbl %>% roc_auc( truth = class, .pred_PS)

rf_training_pred_tbl %>% accuracy(truth = class, .pred_class)

```

Calculate the out-of-sample test data predictions.


```{r calculate_oos_testing_stats}
#| echo: true


rf_testing_pred_tbl <- predict(rf_fit, cell_test_tbl) %>% 
  bind_cols(predict(rf_fit, cell_test_tbl, type = "prob")) %>% 
  bind_cols(cell_test_tbl %>% select(class))

rf_testing_pred_tbl %>% glimpse()

rf_testing_pred_tbl %>% roc_auc( truth = class, .pred_PS)

rf_testing_pred_tbl %>% accuracy(truth = class, .pred_class)

```


## Fit Model with Resampling


```{r fit_model_resampling}
#| echo: true


set.seed(345)

folds <- vfold_cv(cell_train_tbl, v = 10)

folds %>% glimpse()


rf_wf <- workflow() %>%
  add_model(rf_mod) %>%
  add_formula(class ~ .)

set.seed(456)

rf_fit_rs <- rf_wf %>% 
  fit_resamples(folds)

rf_fit_rs %>% print()


collect_metrics(rf_fit_rs)


rf_testing_pred_tbl %>% roc_auc( truth = class, .pred_PS)
rf_testing_pred_tbl %>% accuracy(truth = class, .pred_class)
```


# Workshop 4 - Tune Model Parameters


```{r create_tree_model_tuning}
#| echo: true


tune_spec <- decision_tree(
    cost_complexity = tune(),
    tree_depth = tune()
    ) %>% 
  set_engine("rpart") %>% 
  set_mode("classification")

tune_spec %>% print()
```


## Model Tuning on Grid

We now want to set up the tuning process on a grid.

```{r model_tuning_grid}
#| echo: true


tree_grid_tbl <- grid_regular(
  cost_complexity(),
  tree_depth(),
  levels = 5
  )

tree_grid_tbl %>% print()

tree_grid_tbl %>% count(tree_depth)

set.seed(234)

cell_folds <- vfold_cv(cell_train_tbl)

cell_folds %>% glimpse()
```



```{r tree_tuning_workflow}
#| echo: true

set.seed(345)

tree_wf <- workflow() %>%
  add_model(tune_spec) %>%
  add_formula(class ~ .)

tree_res <- tree_wf %>% 
  tune_grid(
    resamples = cell_folds,
    grid      = tree_grid_tbl
    )

tree_res %>% print()
tree_res %>% collect_metrics()
```

We now want to plot the outputs of these fits.

```{r tree_tuning_metrics_plots}
#| echo: true


tree_res_tbl <- tree_res %>%
  collect_metrics() %>%
  mutate(
    tree_depth = factor(tree_depth)
    )

ggplot(
    tree_res_tbl,
    aes(cost_complexity, mean, color = tree_depth)
    ) +
  geom_line(size = 1.5, alpha = 0.6) +
  geom_point(size = 2) +
  facet_wrap(~ .metric, scales = "free", nrow = 2) +
  scale_x_log10(labels = scales::label_number()) +
  scale_color_viridis_d(option = "plasma", begin = .9, end = 0)

tree_res %>%
  show_best("accuracy")

best_tree <- tree_res %>%
  select_best("accuracy")
```

Now that we have chosen the 'best' fit, we can finish off our workflow to
choose that one.

```{r create_final_workflow}
final_wf <- tree_wf %>% 
  finalize_workflow(best_tree)

final_wf %>% print()
```


## Create Final Fit

```{r create_tree_model_final_fit}
#| echo: true


final_fit <- final_wf %>%
  last_fit(cell_split) 

final_fit %>%
  collect_metrics()

final_fit %>%
  collect_predictions() %>% 
  roc_curve(class, .pred_PS) %>% 
  autoplot()
```


We now want to save this final version and visualise the tree where possible


```{r extract_visualise_final_tree}
#| echo: true


final_tree <- extract_workflow(final_fit)
final_tree %>% print()


final_tree %>%
  extract_fit_engine() %>%
  rpart.plot(roundint = FALSE)

final_tree %>% 
  extract_fit_parsnip() %>% 
  vip()

args(decision_tree)
```


# Workshop 5 - Case Study

## Load Hotel Data


```{r load_hotel_data}
#| echo: true


hotels_tbl <- read_csv(
    file = 'https://tidymodels.org/start/case-study/hotels.csv'
    ) %>%
  mutate(
    across(where(is.character), as.factor)
    )

hotels_tbl %>% glimpse()
```

## Data Splitting and Resampling


```{r data_split_resampling}
set.seed(123)

splits <- initial_split(
  hotels_tbl,
  strata = children
  )

hotel_other_tbl <- training(splits)
hotel_test_tbl  <- testing(splits)

# training set proportions by children
hotel_other_tbl %>% 
  count(children) %>% 
  mutate(prop = n/sum(n))


# test set proportions by children
hotel_test_tbl  %>% 
  count(children) %>% 
  mutate(prop = n/sum(n))


set.seed(234)

val_set <- validation_split(
  hotel_other_tbl, 
  strata = children, 
  prop = 0.80
  )

val_set %>% print()
```


## First Model: Penalized Logistic Regression

We first fit our model using penalized logistic regression.

```{r casestudy_fit_first_model}
#| echo: true


lr_mod <- logistic_reg(
    penalty = tune(),
    mixture = 1
    ) %>%
  set_engine("glmnet")


holidays <- c("AllSouls", "AshWednesday", "ChristmasEve", "Easter", 
              "ChristmasDay", "GoodFriday", "NewYearsDay", "PalmSunday")

lr_recipe <- recipe(
    children ~ .,
    data = hotel_other_tbl
    ) %>% 
  step_date(arrival_date) %>% 
  step_holiday(arrival_date, holidays = holidays) %>% 
  step_rm(arrival_date) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_zv(all_predictors()) %>% 
  step_normalize(all_predictors())

lr_workflow <- workflow() %>% 
  add_model(lr_mod) %>% 
  add_recipe(lr_recipe)
```

We now want to fit the model using the grid.

```{r casestudy_model1_tune_grid}
#| echo: true


lr_reg_grid_tbl <- tibble(penalty = 10^seq(-4, -1, length.out = 30))

lr_res <- lr_workflow %>% 
  tune_grid(
    val_set,
    grid    = lr_reg_grid_tbl,
    control = control_grid(save_pred = TRUE),
    metrics = metric_set(roc_auc)
    )


plot_tbl <- lr_res %>% 
  collect_metrics()

ggplot(plot_tbl, aes(x = penalty, y = mean)) + 
  geom_point() + 
  geom_line() + 
  ylab("Area under the ROC Curve") +
  scale_x_log10(labels = scales::label_number())


top_models_tbl <- lr_res %>% 
  show_best("roc_auc", n = 15) %>% 
  arrange(penalty)

top_models_tbl %>% print()
```

We now want to choose a 'best' model.

```{r casestudy_model1_best_model}
#| echo: true


lr_best_tbl <- lr_res %>% 
  collect_metrics() %>% 
  arrange(penalty) %>% 
  slice(12)

lr_auc <- lr_res %>% 
  collect_predictions(parameters = lr_best_tbl) %>% 
  roc_curve(children, .pred_children) %>% 
  mutate(model = "Logistic Regression")

autoplot(lr_auc)
```



## Second Model - Random Forests with ranger

```{r casestudy_second_model_workflow}
#| echo: true


rf_mod <- rand_forest(
    mtry = tune(),
    min_n = tune(),
    trees = 1000
    ) %>% 
  set_engine("ranger", num.threads = getOption("mc.cores")) %>% 
  set_mode("classification")


rf_recipe <- recipe(
    children ~ .,
    data = hotel_other_tbl
    ) %>% 
  step_date(arrival_date) %>% 
  step_holiday(arrival_date) %>% 
  step_rm(arrival_date) 

rf_workflow <- workflow() %>% 
  add_model(rf_mod) %>% 
  add_recipe(rf_recipe)
```


```{r casestudy_second_model_fit_grid}
#| echo: true


extract_parameter_set_dials(rf_mod)


set.seed(345)

rf_res <- rf_workflow %>% 
  tune_grid(
    val_set,
    grid = 25,
    control = control_grid(save_pred = TRUE),
    metrics = metric_set(roc_auc)
    )

rf_res %>% show_best(metric = "roc_auc")

autoplot(rf_res)

rf_best <- rf_res %>% select_best(metric = "roc_auc")

rf_res %>% collect_predictions()
```

Finally, we show some statistics based on this 'best' model.

```{r casestudy_rfmodel_show_auc}
#| echo: true


rf_auc <- rf_res %>% 
  collect_predictions(parameters = rf_best) %>% 
  roc_curve(children, .pred_children) %>% 
  mutate(
    model = "Random Forest"
    )


plot_tbl <- bind_rows(rf_auc, lr_auc)

ggplot(
    plot_tbl,
    aes(x = 1 - specificity, y = sensitivity, col = model)
    ) + 
  geom_path(lwd = 1.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal() + 
  scale_color_viridis_d(option = "plasma", end = .6)
```



## Final Fit

```{r casestudy_finalfit_workflow}
#| echo: true


# the last model
last_rf_mod <- rand_forest(mtry = 8, min_n = 7, trees = 1000) %>% 
  set_engine("ranger", num.threads = getOption("mc.cores"), importance = "impurity") %>% 
  set_mode("classification")

# the last workflow
last_rf_workflow <- rf_workflow %>% 
  update_model(last_rf_mod)

# the last fit
set.seed(345)

last_rf_fit <- last_rf_workflow %>% last_fit(splits)

last_rf_fit %>% print()

last_rf_fit %>% collect_metrics()
```


We now want to look for variable importance

```{r casestudy_lastfit_vip}
#| echo: true


last_rf_fit %>% 
  extract_fit_parsnip() %>% 
  vip(num_features = 20)


last_rf_fit %>% 
  collect_predictions() %>% 
  roc_curve(children, .pred_children) %>% 
  autoplot()
```










# R Environment

```{r show_session_info}
#| echo: false


options(width = 120L)
sessioninfo::session_info()
options(width = 80L)
```
